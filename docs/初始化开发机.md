# 初始化文档

## 初始化用户

### 创建用户

```
useradd dougzheng
passwd dougzheng
```

### 配置免密 sudo

先 `chmod u+w /etc/sudoers` ，

然后 `vim /etc/sudoers` ，添加一行：

```
## Allow root to run any commands anywhere
root ALL=(ALL) ALL
dougzheng ALL=(ALL) NOPASSWD:ALL # 添加的行
```

最后 `chmod u-w /etc/sudoers` 。

## hadoop 使用

### hosts 配置

登录一台机器作为 master 进行配置。

`vim /etc/hostname` 修改主机名。

`vim /etc/hosts` 中添加 ip 和主机名映射，即 `hadoop101, hadoop102, hadoop103` 的 ip 。

`ping hadoop101` 检查是否生效。

### 免密登录配置

依然在家目录 `.ssh` 目录下进行。

```
ssh-keygen # 一路回车，默认即可
ssh-copy-id hadoop101
ssh-copy-id hadoop102
ssh-copy-id hadoop103
```

### 安装 jdk

安装切到 root 进行。

`scp hadoop.zip username@ip:path` 上传本地的 `hadoop.zip` 压缩包。

`yum install unzip` 安装解压命令然后 `unzip hadoop.zip` 。

安装 jdk ：

```
mkdir /usr/local/jdk
tar -zxvf jdk-8u211-linux-x64.tar.gz -C /usr/local/jdk # 解压到 /usr/local/jdk
```

配置环境变量，在 `/etc/profile` 下面添加：

```
export JAVA_HOME=/usr/local/jdk/jdk1.8.0_211
export PATH=$JAVA_HOME/bin:$PATH
```

`source /etc/profile` 生效后，`java -version` 查看版本。

最后使用 `scp` 同步配置到其他主机。 

### 安装 zookeeper

```
mkdir /usr/local/zookeeper
tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /usr/local/zookeeper
```

配置环境变量，在 `/etc/profile` 下面添加：

```
export ZK_HOME=/usr/local/zookeeper/apache-zookeeper-3.5.7-bin
export PATH=$ZK_HOME/bin:$PATH
```

创建 `zkData` 目录和 `myid` ：

```
mkdir -p /data0/zkData
cd /data0/zkData/
touch myid
echo 1 > myid # hadoop101写入1，hadoop102写入2，...
chown -R dougzheng:dougzheng /data0
```

配置 `zoo.cfg` ：

```
cd $ZK_HOME/conf
cp zoo_sample.cfg zoo.cfg
vim zoo.cfg
```

在 `zoo.cfg` 中作如下修改：

```
dataDir=/data0/zkData # 修改dataDir
# 新增以下配置
server.1=hadoop101:2888:3888
server.2=hadoop102:2888:3888
server.3=hadoop103:2888:3888
```

修改权限：

```
chown -R dougzheng:dougzheng $ZK_HOME
```

最后同步修改所有主机。

### 启动 zookeeper

`su - dougzheng` 切换用户。

在各主机上分别运行 `zkServer.sh start` ，最后 `zkServer.sh status` 查看启动状态，应有 1 个 leader 、2 个 follower 。

可运行 `zkServer.sh start-foreground` 查看运行日志。

### HDFS HA 搭建

```
mkdir /usr/local/hadoop
tar -zxvf hadoop-3.1.3.tar.gz -C /usr/local/hadoop
```

配置环境变量，在 `/etc/profile` 下面添加：

```
export HADOOP_HOME=/usr/local/hadoop/hadoop-3.1.3
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
```

配置 `$HADOOP_HOME/etc/hadoop/` 下的 `hdfs-site.xml` 、`core-site.xml` 、 `yarn-site.xml` 和 `workers` 。

配置 `$HADOOP_HOME/sbin/` 下的 `start-dfs.sh` 、`stop-dfs.sh` 、`start-yarn.sh` 和 `stop-yarn.sh` 。

在 `$HADOOP_HOME/etc/hadoop/hadoop-env.sh` 中加入 `JAVA_HOME` 环境变量。

修改权限：

```
chown -R dougzheng:dougzheng $HADOOP_HOME 
```

最后分发文件到各机器上。

### 启动集群

删除残余文件：

```
rm -rf /usr/local/hadoop/hadoop-3.1.3/logs && rm -rf /tmp/hadoop* && rm -rf /data0/dfs
```

首先在全部机器上启动 `journalnode` ：

```
hdfs --daemon start journalnode
```

在 `nn1` 上格式化 `namenode` ：

```
hdfs namenode -format
```

在 `nn1` 上启动 `namenode` ：

```
hdfs --daemon start namenode
```

在其他机器上同步 `nn1` 并启动 `namenode` ：

```
hdfs namenode -bootstrapStandby && hdfs --daemon start namenode
```

关闭所有 hdfs 服务：

```
stop-all.sh
```

初始化 HA 在 zookeeper 中状态：

```
hdfs zkfc -formatZK
```

启动集群服务：

```
start-all.sh
```

### 跨集群拷贝

`vim $HADOOP_HOME/etc/hadoop/hdfs-site.xml` ，添加以下配置：

```
<property>
    <name>dfs.client.use.datanode.hostname</name>
    <value>true</value>
</property>
```

这个配置在于让服务端 namenode 返回 datanode 节点的主机名而非 ip ，若不配置则服务端 namenode 会返回 datanode 的内网 ip ，客户端无法访问，进而连接超时。

再 `vim /etc/hosts` ，添加服务端（ip，主机名 ）映射：

```
159.75.xxx.xxx   doug-hadoop101  doug-hadoop101
159.75.xxx.xxx   doug-hadoop102  doug-hadoop102
159.75.xxx.xxx   doug-hadoop103  doug-hadoop103
```

这一步使得客户端能够解析服务端返回的 datanode 的主机名，访问 datanode 的公网地址（服务端需开放 namenode 控制访问 8020 端口，以及 datanode 数据传输 9866 端口）。

最后，集群间拷贝数据：

```
hadoop distcp hdfs://doug-hadoop101:8020/test.txt /test_download.txt
```

## hive 使用

### 安装 mysql

切换到 root 操作。

```
yum remove mysql-libs
yum install libaio
yum install autoconf
rpm -ivh mysql-community-common-5.7.25-1.el7.x86_64.rpm
rpm -ivh mysql-community-libs-5.7.25-1.el7.x86_64.rpm
rpm -ivh mysql-community-libs-compat-5.7.25-1.el7.x86_64.rpm
rpm -ivh mysql-community-client-5.7.25-1.el7.x86_64.rpm
rpm -ivh mysql-community-server-5.7.25-1.el7.x86_64.rpm
```

`systemctl start mysqld` 启动 mysql 服务。

`cat /var/log/mysqld.log | grep password` 查看 mysql 初始密码。

### 配置 mysql

`mysql -u root -p` ，输入密码，登录 mysql 。

配置新密码：

```
set global validate_password_length=4;
set global validate_password_policy=0;
set password=password("000000");
```

配置 host ：

```
use mysql
select user, host from user;
update user set host="%" where user="root";
flush privileges;
```

配置新用户：

```
create user 'dougzheng'@'%' identified by 'typeyourpassword';
GRANT ALL ON *.* TO 'dougzheng'@'%';
```

### 安装 hive

```
mkdir /usr/local/hive
tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /usr/local/hive
```

配置环境变量，在 `/etc/profile` 下面添加：

```
# hive
export HIVE_HOME=/usr/local/hive/apache-hive-3.1.2-bin
export PATH=$HIVE_HOME/bin:$PATH
```

解决 jar 包冲突：

```
mv $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.jar $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.jar.bak
rm $HIVE_HOME/lib/guava*.jar && cp $HADOOP_HOME/share/hadoop/common/lib/guava*.jar $HIVE_HOME/lib/
```

拷贝 jdbc 驱动：

```
cp /opt/software/mysql-connector-java-5.1.48.jar $HIVE_HOME/lib
```

配置 `hive-site.xml` ：

```
vim $HIVE_HOME/conf/hive-site.xml # 内容略
```

### 启动 hive

新建 hive 元数据库：

```
mysql -u dougzheng -p
create database metastore;
```

修改权限：

```
chown -R dougzheng:dougzheng $HIVE_HOME
```

初始化 hive 元数据库：

```
su - dougzheng
schematool -initSchema -dbType mysql -verbose
```

查看 hive 数据库：

```
hive
show databases;
```

### 配置 hive

修改日志存放目录：

```
mv $HIVE_HOME/conf/hive-log4j2.properties{.template,}
```

`vim hive-log4j.properties` ，修改 `hive.log.dir` 属性。

## shell 脚本

### 文件分发

e.g. `./xsync.sh /usr/local/hadoop` 

```
#!/bin/bash
ips=(hadoop101 hadoop102 hadoop103)
for ip in ${ips[@]}
do
  for file in $@
    do          
      if [[ -e $file ]] 
      then      
        pdir=$(cd -P $(dirname $file); pwd)
        fname=$(basename $file) 
        ssh $ip "mkdir -p $pdir"
        rsync -av $pdir/$fname $ip:$pdir
        echo $pdir              
      else      
        echo $file does not exists! 
      fi      
    done        
done
```

### hdfs 格式化

e.g. `./format.sh` 

```
#!/bin/bash
ips=(hadoop101 hadoop102 hadoop103)

for ip in ${ips[@]}
do
ssh -T $ip <<EOF
rm -rf $HADOOP_HOME/logs /tmp/hadoop* /data0/dfs /data0/hadoop /data1
mkdir -p /data0/hadoop/hdfs/name
EOF
done

for ip in ${ips[@]}
do
ssh -T $ip <<EOF
hdfs --daemon start journalnode
EOF
done

ssh -T ${ips[0]} <<EOF
hdfs namenode -format <<YON
Y
YON
hdfs --daemon start namenode
EOF

ssh -T ${ips[1]} <<EOF
hdfs namenode -bootstrapStandby
hdfs --daemon start namenode
EOF

ssh -T ${ips[2]} <<EOF
hdfs namenode -bootstrapStandby
hdfs --daemon start namenode
EOF

ssh -T ${ips[0]} <<EOF
stop-all.sh
hdfs zkfc -formatZK <<YON
Y
Y
YON
start-all.sh
EOF
```

### zookeeper 启动/关闭

e.g. `./zk.sh start` 、`./zk.sh status` 

```
#!/bin/bash
ips=(hadoop101 hadoop102 hadoop103)
cmd=$1
[[ $cmd == status ]] && out=/dev/stdout || out=/dev/null
echo command is $cmd
for ip in ${ips[@]}
do
echo remote to $ip.
ssh $ip >$out 2>&1 <<EOF
    cd
    zkServer.sh $cmd
    exit
EOF
done
echo command done.
```

### hdfs 安装配置

需要预先配置好 `hadoop101` 到其他主机的 ssh 免密登录，准备 `config` 文件到脚本同一目录下，内容格式如下：

```
172.16.0.15	doug-hadoop101
172.16.0.3	doug-hadoop102
172.16.0.14	doug-hadoop103
```

`./install-hdfs.sh` 运行，输入的 user 是安装使用的用户，如 `hadoop` 或者 `root` 用户，需要预先创建好。

```
#!/bin/bash
host_num=3
ips=()
hosts=()

if [[ ! -e ./config ]]; then
  echo "ERR: config not exsits."
  echo 'INFO: config format is $ip $hostname'
  exit
fi

if [[ `whoami` != root ]]; then
  echo "ERR: root required"
  exit
fi

echo -e "input user to install: \c"
read username

echo "INFO: user is $username"

[[ $username == root ]] && userhome=/root || userhome=/home/$username
echo $userhome

user_exist=1
id -u $username >/dev/null 2>&1
if (($? != 0)); then
  echo "ERR: user not exists."
  exit
fi

install_path=/opt/software
install_pkg=(
  jdk-8u211-linux-x64.tar.gz
  hadoop-3.1.3.tar.gz
  apache-zookeeper-3.5.7-bin.tar.gz
)

is_ok=1
for pkg in ${install_pkg[@]}
do
  if [[ ! -e $install_path/$pkg ]]; then
    echo $install_path/$pkg not found
    is_ok=0
  fi
done

if ((is_ok == 0)); then
  exit
fi

main_host=`ifconfig eth0 | grep cast | awk -F ' ' '{print $2}'`
echo "local host: $main_host"

i=0
while read line
do
  if ((i >= host_num)); then
    break
  fi
  na=($line)
  ips[$i]=${na[0]}
  hosts[$i]=${na[1]}
  ((++i))
done <config

if [[ $main_host != ${ips[0]} ]]; then
  echo ips[0] in config is not equal to $main_host
  exit
fi

hosts_config=`cat config`

for ((i=0; i < host_num; ++i))
do
  echo ${ips[$i]} ${hosts[$i]}
done

echo -e "ready to install?(Y/N): \c"
read ret
if [[ $ret != "Y" ]]; then
  exit 
fi

cat /dev/null > /etc/hosts
echo "$hosts_config" > /etc/hosts
echo ${hosts[$i]} > /etc/hostname

sed -i '/# === auto gen by install-hdfs.sh start ===/,/# === auto gen by install-hdfs.sh end ===/d' /etc/profile

echo \
'# === auto gen by install-hdfs.sh start ===
# JAVA
export JAVA_HOME=/usr/local/jdk/jdk1.8.0_211
export PATH=$JAVA_HOME/bin:$PATH
# ZOOKEEPER
export ZK_HOME=/usr/local/zookeeper/apache-zookeeper-3.5.7-bin
export PATH=$ZK_HOME/bin:$PATH
# HADOOP
export HADOOP_HOME=/usr/local/hadoop/hadoop-3.1.3
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
# === auto gen by install-hdfs.sh end ===' \
>> /etc/profile
source /etc/profile

if [[ ! -e /opt/software ]]; then mkdir /opt/software; fi
cd /opt/software

if [[ -e /usr/local/jdk ]]; then rm -rf /usr/local/jdk; fi
mkdir /usr/local/jdk
tar -zxvf jdk-8u211-linux-x64.tar.gz -C /usr/local/jdk

if [[ -e /usr/local/zookeeper ]]; then rm -rf /usr/local/zookeeper; fi
mkdir /usr/local/zookeeper
tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /usr/local/zookeeper

if [[ -e /data0/zkData ]]; then rm -rf /data0; fi
mkdir -p /data0/zkData
cd /data0/zkData/
touch myid
echo "1" > myid
cd $ZK_HOME/conf
cp zoo_sample.cfg zoo.cfg

sed -i "s/^dataDir.*/dataDir=\/data0\/zkData/g" zoo.cfg
for ((i=0; i < host_num; ++i)); do
# ((i == 0)) && thost=0.0.0.0 || thost=${hosts[$i]};
echo "server.$((i+1))=${hosts[$i]}:2888:3888" >> zoo.cfg; done

cd /opt/software
if [[ -e /usr/local/hadoop ]]; then rm -rf /usr/local/hadoop; fi
mkdir /usr/local/hadoop
tar -zxvf hadoop-3.1.3.tar.gz -C /usr/local/hadoop

cd $HADOOP_HOME/etc/hadoop
cat > hdfs-site.xml <<XML
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->
<configuration>

<!--配置副本数-->
<property>
  <name>dfs.replication</name>
  <value>3</value>
</property>

<!--配置nameservice-->
<property>
  <name>dfs.nameservices</name>
  <value>mycluster</value>
</property>

<!--配置多NamenNode-->
<property>
  <name>dfs.ha.namenodes.mycluster</name>
  <value>nn1,nn2,nn3</value>
</property>
<property>
  <name>dfs.namenode.rpc-address.mycluster.nn1</name>
  <value>${hosts[0]}:8020</value>
</property>
<property>
  <name>dfs.namenode.rpc-address.mycluster.nn2</name>
  <value>${hosts[1]}:8020</value>
</property>
<property>
  <name>dfs.namenode.rpc-address.mycluster.nn3</name>
  <value>${hosts[2]}:8020</value>
</property>

<!--为NamneNode设置HTTP服务监听-->
<property>
  <name>dfs.namenode.http-address.mycluster.nn1</name>
  <value>${hosts[0]}:9870</value>
</property>
<property>
  <name>dfs.namenode.http-address.mycluster.nn2</name>
  <value>${hosts[1]}:9870</value>
</property>
<property>
  <name>dfs.namenode.http-address.mycluster.nn3</name>
  <value>${hosts[2]}:9870</value>
</property>

<!--指定jn存储路径-->
<property>
  <name>dfs.journalnode.edits.dir</name>
  <value>/data0/dfs/journal/data</value>
</property>

<!--配置jn节点，该节点用于各NameNode节点通信-->
<property>
  <name>dfs.namenode.shared.edits.dir</name>
  <value>qjournal://${hosts[0]}:8485;${hosts[1]}:8485;${hosts[2]}:8485/mycluster</value>
</property>

<!--指定namenode元数据信息存储位置-->
<property>
  <name>dfs.namenode.name.dir</name>
  <value>/data0/hadoop/hdfs/name</value>
</property>

<!--指定datanode数据存储位置--> 
<property>
  <name>dfs.datanode.data.dir</name>
  <value>/data0/hadoop/hdfs/data</value>
</property>

<!--配置HDFS客户端联系Active NameNode节点的Java类-->
<property>
  <name>dfs.client.failover.proxy.provider.mycluster</name>
  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>

 <!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 -->
<property>
  <name>dfs.ha.fencing.methods</name>
  <value>
    sshfence
    shell(/bin/true)
  </value>
</property>

<!-- 使用隔离机制时需要ssh无秘钥登录-->
<property>
  <name>dfs.ha.fencing.ssh.private-key-files</name>
  <value>$userhome/.ssh/id_rsa</value>
</property>

<!-- 关闭权限检查-->
<property>
  <name>dfs.permissions.enable</name>
  <value>false</value>
</property>

<!--配置故障自动转义-->
 <property>
   <name>dfs.ha.automatic-failover.enabled</name>
   <value>true</value>
 </property>

<property>
    <name>dfs.client.use.datanode.hostname</name>
    <value>true</value>
    <description>only cofig in clients</description>
 </property>

</configuration>
XML

cat > core-site.xml <<XML
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!-- Put site-specific property overrides in this file. -->

<configuration>

<!--指定defaultFS-->
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://mycluster</value>
</property>

<!--配置hadoop运行时临时文件-->
<property>
  <name>hadoop.tmp.dir</name>
  <value>/data0/dfs/journal/tmp</value>
</property>

<!--配置zookeeper地址-->
<property>
  <name>ha.zookeeper.quorum</name>
  <value>${hosts[0]}:2181,${hosts[1]}:2181,${hosts[2]}:2181</value>        
</property>

</configuration>
XML

cat > yarn-site.xml <<XML
<?xml version="1.0"?>
<configuration>

<!-- Site specific YARN configuration properties -->

<!--yarn 高可用配置-->
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>

<property>
  <name>yarn.resourcemanager.ha.enabled</name>
  <value>true</value>
</property>

<property>
  <name>yarn.resourcemanager.cluster-id</name>
  <value>cluster1</value>
</property>

<property>
  <name>yarn.resourcemanager.ha.rm-ids</name>
  <value>rm1,rm2</value>
</property>

<property>
  <name>yarn.resourcemanager.hostname.rm1</name>
  <value>${hosts[0]}</value>
</property>

<property>
  <name>yarn.resourcemanager.hostname.rm2</name>
  <value>${hosts[2]}</value>
</property>

<property>
  <name>yarn.resourcemanager.webapp.address.rm1</name>
  <value>${hosts[0]}:8088</value>
</property>

<property>
  <name>yarn.resourcemanager.webapp.address.rm2</name>
  <value>${hosts[2]}:8088</value>
</property>

<property>
  <name>hadoop.zk.address</name>
  <value>${hosts[0]}:2181,${hosts[1]}:2181,${hosts[2]}:2181</value>        
</property>

 <!--启用自动恢复-->
<property>
  <name>yarn.resourcemanager.recovery.enabled</name>
  <value>true</value>
</property>

<!--指定resourcemanager的状态信息存储在zookeeper集群-->
<property>
  <name>yarn.resourcemanager.store.class</name>
  <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
</property>

</configuration>
XML

cat /dev/null > workers
for ((i=0; i < host_num; ++i))
do
  echo ${hosts[$i]} >> workers
done

cd $HADOOP_HOME/sbin
sed -i "1a \\
HDFS_DATANODE_USER=$username\\
HADOOP_SECURE_DN_USER=hdfs\\
HDFS_NAMENODE_USER=$username\\
HDFS_SECONDARYNAMENODE_USER=$username\\
HDFS_JOURNALNODE_USER=$username\\
HDFS_ZKFC_USER=$username" start-dfs.sh stop-dfs.sh
sed -i "1a \\
YARN_RESOURCEMANAGER_USER=$username\\
HADOOP_SECURE_DN_USER=yarn\\
YARN_NODEMANAGER_USER=$username" start-yarn.sh stop-yarn.sh
echo 'export JAVA_HOME=/usr/local/jdk/jdk1.8.0_211' >> $HADOOP_HOME/etc/hadoop/hadoop-env.sh

chown -R $username:$username /data0
chown -R $username:$username $JAVA_HOME
chown -R $username:$username $ZK_HOME
chown -R $username:$username $HADOOP_HOME

files=(
  /etc/hosts
  /etc/profile
  $JAVA_HOME
  $HADOOP_HOME
  $ZK_HOME
  /data0
)
echo "${hosts[@]}"
for ((i=1; i < host_num; ++i))
do
  host=${hosts[$i]}
  for file in ${files[@]}
    do          
      if [[ -e $file ]]; then      
        pdir=$(cd -P $(dirname $file); pwd)
        fname=$(basename $file) 
        ssh $host "mkdir -p $pdir"
        rsync -av $pdir/$fname $host:$pdir
        echo $pdir              
      else      
        echo $file does not exists! 
      fi      
    done        
done

for ((i=1; i < host_num; ++i))
do
host=${hosts[$i]}
ssh -T $host <<EOF
echo $(($i+1)) > /data0/zkData/myid 
echo $host > /etc/hostname
EOF
done

su - $username
```

